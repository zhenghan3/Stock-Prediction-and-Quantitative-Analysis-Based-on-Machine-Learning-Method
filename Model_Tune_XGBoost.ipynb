{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Tune_XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjeI9d2JGcZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4579665b-01ba-4f23-feb4-a787008291ac"
      },
      "source": [
        "# Generate the corresponding three optimal models of the the three different lengths of the training sets.\n",
        "\n",
        "# Load the database from google drive\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Unzip the database from google drive to Colab\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Database.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/Database\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Unzip the required .py files from google drive to Colab\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Py.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n",
        "\n",
        "# From Google drive to Colab\n",
        "!cp -rf \"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Database/result\" '/content/Database'\n",
        "\n",
        "!pip install tushare\n",
        "!pip install jqdatasdk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "Collecting tushare\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/96/d99c405aa2490205c08b3f75335014cde4b4bc4637d637c1dd8b64d6be64/tushare-1.2.60-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: bs4>=0.0.1 in /usr/local/lib/python3.6/dist-packages (from tushare) (0.0.1)\n",
            "Collecting websocket-client>=0.57.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tushare) (2.23.0)\n",
            "Collecting simplejson>=3.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tushare) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4>=0.0.1->tushare) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.57.0->tushare) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->tushare) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->tushare) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->tushare) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->tushare) (3.0.4)\n",
            "Installing collected packages: websocket-client, simplejson, tushare\n",
            "Successfully installed simplejson-3.17.2 tushare-1.2.60 websocket-client-0.57.0\n",
            "Collecting jqdatasdk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/a1/bde42d53633ad9ed5ddb1fbf5b482200ceb4eaa8087dbba63420a3137c29/jqdatasdk-1.8.1-py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hCollecting pandas<=0.25.3,>=0.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from jqdatasdk) (1.18.5)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.6/dist-packages (from jqdatasdk) (1.3.18)\n",
            "Collecting thriftpy2>=0.3.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/f0/9bf08e6b5983aa6a6103818da21eadfaea1ad99ec9882be3e75a30e8e9ff/thriftpy2-0.4.11.tar.gz (498kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=0.4.7 in /usr/local/lib/python3.6/dist-packages (from jqdatasdk) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from jqdatasdk) (2.23.0)\n",
            "Collecting pymysql>=0.7.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/57/af502e0e113f139b3f3add4f1efba899a730a365d2264d476e85b9591da5/PyMySQL-0.10.0-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jqdatasdk) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas<=0.25.3,>=0.16.2->jqdatasdk) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<=0.25.3,>=0.16.2->jqdatasdk) (2018.9)\n",
            "Collecting ply<4.0,>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->jqdatasdk) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->jqdatasdk) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->jqdatasdk) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->jqdatasdk) (1.24.3)\n",
            "Building wheels for collected packages: thriftpy2\n",
            "  Building wheel for thriftpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.4.11-cp36-cp36m-linux_x86_64.whl size=930441 sha256=eb3df637c47a36b37f7bedb71366371c74fc1e66363ccfbe0dfb00fca624d38b\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/ae/4c/216f0f9a8a65dcb81e633b70a77740a879e3e923a523eeb315\n",
            "Successfully built thriftpy2\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas, ply, thriftpy2, pymysql, jqdatasdk\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "Successfully installed jqdatasdk-1.8.1 pandas-0.25.3 ply-3.11 pymysql-0.10.0 thriftpy2-0.4.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Tfb0hw4KTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Database/result\"):\n",
        "    os.makedirs(\"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Database/result\")\n",
        "\n",
        "# From Colab to Google drive\n",
        "!cp -rf '/content/Database/result' \"/content/drive/My Drive/Colab Notebooks/Dissertation/Factor/Database\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RyHcT-EGPRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "outputId": "1dcdb547-ee20-41af-ce9c-a5133ea35cda"
      },
      "source": [
        "# Take the previous 1-year data as the training set and generate the corresponding optimal model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV,cross_validate\n",
        "\n",
        "from Data_Cleaning import save_to_file,read_from_file,get_dataset_bydate,get_date_list,authorization_jq\n",
        "\n",
        "# Plot the \"n_estimators vs auc\" curve figure\n",
        "def plot_cvresult(cvresult,n_estimators,learning_rate,split_part=100,overwrite=False):\n",
        "    plt.close()\n",
        "    global training_year\n",
        "    split_num=round(n_estimators/split_part)\n",
        "    # plot\n",
        "    test_means = cvresult['test-auc-mean'][::split_num]\n",
        "    test_stds = cvresult['test-auc-std'][::split_num]\n",
        "\n",
        "    train_means = cvresult['train-auc-mean'][::split_num]\n",
        "    train_stds = cvresult['train-auc-std'][::split_num]\n",
        "\n",
        "    x_axis = range(0, n_estimators)[::split_num]\n",
        "    plt.errorbar(x_axis, test_means, yerr=test_stds, label='Test')\n",
        "    plt.errorbar(x_axis, train_means, yerr=train_stds, label='Train')\n",
        "    plt.title(\"n_estimators vs auc\")\n",
        "    plt.xlabel('n_estimators')\n",
        "    plt.ylabel('auc')\n",
        "\n",
        "    if (not overwrite) and os.path.exists('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png'):\n",
        "        plt.show()\n",
        "        return False\n",
        "    plt.savefig('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the feature importance figures of the model\n",
        "def plot_importance_custom(model, importance_type='weight',plot_bar_label=False, plot_importance_label=True, max_num_features=-1):\n",
        "    plt.close()\n",
        "    fig, ax = plt.subplots(figsize=(15, 15))\n",
        "    # Plot the feature importance histogram\n",
        "    if plot_bar_label:\n",
        "        plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the XGBoost built-in plot_importance figure\n",
        "    if plot_importance_label:\n",
        "        if max_num_features == -1:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type)\n",
        "        else:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type, max_num_features=max_num_features)\n",
        "        plt.show()\n",
        "\n",
        "# Function to fit and predict the model\n",
        "# cross validation enabled to obtain the best n_estimators if useTrainCV==True\n",
        "def model_fit(model, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "    global X_train,Y_train,training_year\n",
        "    if useTrainCV:\n",
        "        xgb_param = model.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(X_train.values, label=Y_train.values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
        "                          metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=50)\n",
        "        n_estimators=cvresult.shape[0]\n",
        "        model.set_params(n_estimators=n_estimators)\n",
        "\n",
        "        save_to_file(cvresult,'cvresult_'+str(n_estimators)+'_'+str(model.learning_rate)+'_'+str(training_year),'Database/result/'+str(training_year)+'/'+'n_estimators',overwrite=False)\n",
        "        plot_cvresult(cvresult,n_estimators,model.learning_rate)\n",
        "\n",
        "    # Fit the model on the data\n",
        "    model.fit(X_train, Y_train, eval_metric='auc')\n",
        "\n",
        "    # Predict the training set:\n",
        "    Y_pred = model.predict(X_train)\n",
        "    Y_predprob = model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "    # Print the model report:\n",
        "    print(\"\\nModel Report\")\n",
        "    print(\"Accuracy : %.4g\" % accuracy_score(Y_train.values, Y_pred))\n",
        "    print(\"AUC Score (Train): %f\" % roc_auc_score(Y_train, Y_predprob))\n",
        "\n",
        "    plot_importance_custom(model)\n",
        "\n",
        "# Print the grid search cv results\n",
        "def print_grid_search_cv_results_(grid_search_model):\n",
        "    for index, value in enumerate(grid_search_model.cv_results_['params']):\n",
        "        print(value, \" : \", '%.4f' % grid_search_model.cv_results_['mean_test_score'][index], ' , ',\n",
        "              '%.4f' % grid_search_model.cv_results_['std_test_score'][index])\n",
        "    print('Best Params: ', grid_search_model.best_params_)\n",
        "    print(grid_search_model.best_score_)\n",
        "\n",
        "# Plot the grid search auc figure given the model\n",
        "def plot_grid_search_auc(grid_search_model,para_list,overwrite=False,plot_label=True):\n",
        "    plt.close()\n",
        "    # If there is only 1 parameter in the para_list\n",
        "    if len(para_list)==1:\n",
        "        para_label=list(para_list.keys())[0]\n",
        "        para_range=list(para_list.values())[0]\n",
        "        test_means = grid_search_model.cv_results_['mean_test_score']\n",
        "        test_scores = np.array(test_means)\n",
        "\n",
        "        #print(para_range,test_scores,para_label,sep='\\n')\n",
        "        plt.plot(para_range, test_scores, label=para_label)\n",
        "        plt.legend()\n",
        "        plt.xlabel(para_label)\n",
        "        plt.ylabel('auc')\n",
        "    # Two parameters in the para_list\n",
        "    else:\n",
        "        x_label=list(para_list.keys())[1]\n",
        "        y_label=list(para_list.keys())[0]\n",
        "        x_len=len(list(para_list.values())[1])\n",
        "        y_len=len(list(para_list.values())[0])\n",
        "\n",
        "        grid_visualisation=[]\n",
        "        for pair in grid_search_model.cv_results_['mean_test_score']:\n",
        "            grid_visualisation.append(pair)\n",
        "        grid_visualisation=np.array(grid_visualisation)\n",
        "        grid_visualisation.shape=(y_len,x_len)\n",
        "\n",
        "        sns.heatmap(grid_visualisation,cmap='Blues')\n",
        "\n",
        "        # Switch the x and y axis if length are equal\n",
        "        if not x_len==y_len:\n",
        "            plt.xticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.yticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.xlabel(x_label)\n",
        "            plt.ylabel(y_label)\n",
        "        else:\n",
        "            plt.xticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.yticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.xlabel(y_label)\n",
        "            plt.ylabel(x_label)\n",
        "        \n",
        "    path='Database/result/'+str(training_year)+'/' + str(list(para_list.keys()))+'_'+str(list(para_list.values()))+'_'+str(training_year) + '.png'\n",
        "    if (not overwrite) and os.path.exists(path):\n",
        "        if plot_label:\n",
        "            plt.show()\n",
        "        return False\n",
        "    plt.title(str(list(para_list.keys()))+' vs auc')\n",
        "    plt.savefig(path)\n",
        "    if plot_label:\n",
        "        plt.show()\n",
        "\n",
        "# Grid search on the parameters\n",
        "def grid_search_on_para(para_list,num,re_run=False,print_results=False,overwrite=False):\n",
        "    global xgb_model,X_train,Y_train\n",
        "\n",
        "    if (re_run==False) and (os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')):\n",
        "        grid_search_model = read_from_file('grid_search_model'+str(num)+'_'+str(training_year), 'Database/result/'+str(training_year))\n",
        "\n",
        "    if (re_run) or (not os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')) or re_run:\n",
        "        global xgb_model,X_train,Y_train\n",
        "        grid_search_model = GridSearchCV(estimator = xgb_model,\n",
        "                               param_grid = para_list, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=1)\n",
        "        grid_search_model.fit(X_train,Y_train)\n",
        "        save_to_file(grid_search_model,'grid_search_model'+str(num)+'_'+str(training_year),'Database/result/'+str(training_year),overwrite=True)\n",
        "    # Print results\n",
        "    if print_results:\n",
        "        print_grid_search_cv_results_(grid_search_model)\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite)\n",
        "    else:\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite,plot_label=False)\n",
        "\n",
        "    return grid_search_model\n",
        "\n",
        "# Return the model auc or acc on test set\n",
        "def get_model_accuracy(model,prob_label=False):\n",
        "    global X_train,Y_train,X_test,Y_gold\n",
        "    model.fit(X_train,Y_train)\n",
        "    Y_test=model.predict(X_test)\n",
        "    Y_prob=model.predict_proba(X_test)\n",
        "    accuracy=accuracy_score(Y_gold,Y_test)\n",
        "    if prob_label:\n",
        "        return accuracy,Y_prob\n",
        "    else:\n",
        "        return accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pro=authorization_jq(1)\n",
        "\n",
        "    training_year=1\n",
        "    interval=20\n",
        "    start_date = '2007-01-01'\n",
        "    end_date = '2020-07-15'\n",
        "    base_date='2017-01-24'\n",
        "    date_list = get_date_list(start_date, end_date, base_date, interval=interval)\n",
        "\n",
        "    # Obtain the training set and the test set\n",
        "    training_set, test_set = get_dataset_bydate(base_date,date_list,year=training_year)\n",
        "    X_train=training_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_train=training_set['label']\n",
        "    #Y_train=training_set['label'].apply(lambda x:0 if x==-1 else 1)\n",
        "    X_test=test_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_gold=test_set['label']\n",
        "    #Y_gold=test_set['label'].apply(lambda x:0 if x==-1 else 1)\n",
        "\n",
        "    # Define the classifer\n",
        "    xgb_model = XGBClassifier(\n",
        "            learning_rate =0.1,\n",
        "            n_estimators=200,\n",
        "            max_depth=5,\n",
        "            min_child_weight=1,\n",
        "            gamma=0,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective= 'binary:logistic',\n",
        "            nthread=-1,\n",
        "            scale_pos_weight=1,\n",
        "            seed=27,\n",
        "            tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "            )\n",
        "\n",
        "    # Grid search on learning_rate\n",
        "    para_list10 = {\n",
        "    'learning_rate':[0.05,0.1,0.15,0.2,0.3]\n",
        "    }\n",
        "    grid_search_on_para(para_list10,10)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list11 = {\n",
        "    'learning_rate':[i/100.0 for i in range(1,10,1)],\n",
        "    }\n",
        "    grid_search_on_para(para_list11,11)\n",
        "\n",
        "    xgb_model.learning_rate=0.04\n",
        "    xgb_model.n_estimators=1000\n",
        "\n",
        "    # Find the optimal number of n_estimators\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    xgb_model.n_estimators=200\n",
        "    # Grid search on max_depth and min_child_weight\n",
        "    para_list1 = {\n",
        "        'max_depth': range(3, 10, 2),\n",
        "        'min_child_weight': range(1, 6, 2)\n",
        "    }\n",
        "    grid_search_on_para(para_list1,1)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list2 = {\n",
        "        'max_depth': [6,7,8],\n",
        "        'min_child_weight': [1,2,3,4,5]\n",
        "    }\n",
        "    grid_search_on_para(para_list2,2)\n",
        "\n",
        "    xgb_model.max_depth=8\n",
        "    xgb_model.min_child_weight=4\n",
        "\n",
        "    # Grid search on gamma\n",
        "    para_list3 = {\n",
        "        'gamma':[i/10.0 for i in range(0,5)]\n",
        "    }\n",
        "    grid_search_on_para(para_list3,3)\n",
        "\n",
        "    # Grid search on subsample and colsample_bytree\n",
        "    para_list4 = {\n",
        "        'subsample':[i/10.0 for i in range(6,10)],\n",
        "        'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
        "    }   \n",
        "    grid_search_on_para(para_list4,4)\n",
        "    print(get_model_accuracy(xgb_model))\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list5 = {\n",
        "        'subsample':[i/100.0 for i in range(70,91,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(70,91,5)],\n",
        "    }\n",
        "    grid_search_on_para(para_list5,5,print_results=True,re_run=True,overwrite=True)\n",
        "\n",
        "    xgb_model.subsample=0.8\n",
        "    xgb_model.colsample_bytree=0.8\n",
        "\n",
        "    # Grid search on reg_aplha\n",
        "    para_list6 = {\n",
        "    'reg_alpha':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list6,6)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list7 = {\n",
        "    'reg_alpha':[0.001,0.005,0.01,0.03,0.05]\n",
        "    }\n",
        "    grid_search_on_para(para_list7,7)\n",
        "    #grid_search_on_para(para_list7,7,re_run=True,print_results=True,overwrite=True)\n",
        "\n",
        "    xgb_model.reg_alpha=0.005\n",
        "\n",
        "    # Grid search on reg_lamda\n",
        "    para_list8 = {\n",
        "    'reg_lambda':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list8,8)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list9 = {\n",
        "    'reg_lambda':[0.5,0.8,1,1.5,2,5,10]\n",
        "    }\n",
        "    grid_search_on_para(para_list9,9)\n",
        "\n",
        "    model.kwargs['reg_lamda']=1\n",
        "\n",
        "    xgb_model.n_estimators=5000\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    # Final model concerning the 1-year training set\n",
        "    xgb_model = XGBClassifier(\n",
        "        learning_rate =0.04,\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        min_child_weight=4,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.005,\n",
        "        reg_lamda=1,\n",
        "\n",
        "        objective= 'binary:logistic',\n",
        "        nthread=-1,\n",
        "        scale_pos_weight=1,\n",
        "        seed=27,\n",
        "        tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "        )\n",
        "    print(get_model_accuracy(xgb_model))\n",
        "\n",
        "plt.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "提示：当前环境pandas版本为0.25，get_price与get_fundamentals_continuously接口panel参数将固定为False\n",
            "注意：0.25以上版本pandas不支持panel，如使用该数据结构和相关函数请注意修改\n",
            "auth success \n",
            "0.5529100529100529\n",
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:  7.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'colsample_bytree': 0.7, 'subsample': 0.7}  :  0.7363  ,  0.0263\n",
            "{'colsample_bytree': 0.7, 'subsample': 0.75}  :  0.7331  ,  0.0258\n",
            "{'colsample_bytree': 0.7, 'subsample': 0.8}  :  0.7339  ,  0.0277\n",
            "{'colsample_bytree': 0.7, 'subsample': 0.85}  :  0.7329  ,  0.0274\n",
            "{'colsample_bytree': 0.7, 'subsample': 0.9}  :  0.7340  ,  0.0271\n",
            "{'colsample_bytree': 0.75, 'subsample': 0.7}  :  0.7354  ,  0.0237\n",
            "{'colsample_bytree': 0.75, 'subsample': 0.75}  :  0.7313  ,  0.0285\n",
            "{'colsample_bytree': 0.75, 'subsample': 0.8}  :  0.7325  ,  0.0239\n",
            "{'colsample_bytree': 0.75, 'subsample': 0.85}  :  0.7330  ,  0.0254\n",
            "{'colsample_bytree': 0.75, 'subsample': 0.9}  :  0.7320  ,  0.0265\n",
            "{'colsample_bytree': 0.8, 'subsample': 0.7}  :  0.7322  ,  0.0234\n",
            "{'colsample_bytree': 0.8, 'subsample': 0.75}  :  0.7322  ,  0.0248\n",
            "{'colsample_bytree': 0.8, 'subsample': 0.8}  :  0.7374  ,  0.0266\n",
            "{'colsample_bytree': 0.8, 'subsample': 0.85}  :  0.7329  ,  0.0260\n",
            "{'colsample_bytree': 0.8, 'subsample': 0.9}  :  0.7357  ,  0.0258\n",
            "{'colsample_bytree': 0.85, 'subsample': 0.7}  :  0.7347  ,  0.0264\n",
            "{'colsample_bytree': 0.85, 'subsample': 0.75}  :  0.7327  ,  0.0281\n",
            "{'colsample_bytree': 0.85, 'subsample': 0.8}  :  0.7363  ,  0.0269\n",
            "{'colsample_bytree': 0.85, 'subsample': 0.85}  :  0.7361  ,  0.0275\n",
            "{'colsample_bytree': 0.85, 'subsample': 0.9}  :  0.7350  ,  0.0255\n",
            "{'colsample_bytree': 0.9, 'subsample': 0.7}  :  0.7327  ,  0.0277\n",
            "{'colsample_bytree': 0.9, 'subsample': 0.75}  :  0.7350  ,  0.0284\n",
            "{'colsample_bytree': 0.9, 'subsample': 0.8}  :  0.7339  ,  0.0235\n",
            "{'colsample_bytree': 0.9, 'subsample': 0.85}  :  0.7337  ,  0.0245\n",
            "{'colsample_bytree': 0.9, 'subsample': 0.9}  :  0.7358  ,  0.0243\n",
            "Best Params:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
            "0.7374344452854671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.544973544973545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIPwdMk1Bxeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take the previous 2-year data as the training set and generate the corresponding optimal model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV,cross_validate\n",
        "\n",
        "from Data_Cleaning import save_to_file,read_from_file,get_dataset_bydate,get_date_list,authorization_jq\n",
        "\n",
        "# Plot the \"n_estimators vs auc\" curve figure\n",
        "def plot_cvresult(cvresult,n_estimators,learning_rate,split_part=100,overwrite=False):\n",
        "    plt.close()\n",
        "    global training_year\n",
        "    split_num=round(n_estimators/split_part)\n",
        "    # plot\n",
        "    test_means = cvresult['test-auc-mean'][::split_num]\n",
        "    test_stds = cvresult['test-auc-std'][::split_num]\n",
        "\n",
        "    train_means = cvresult['train-auc-mean'][::split_num]\n",
        "    train_stds = cvresult['train-auc-std'][::split_num]\n",
        "\n",
        "    x_axis = range(0, n_estimators)[::split_num]\n",
        "    plt.errorbar(x_axis, test_means, yerr=test_stds, label='Test')\n",
        "    plt.errorbar(x_axis, train_means, yerr=train_stds, label='Train')\n",
        "    plt.title(\"n_estimators vs auc\")\n",
        "    plt.xlabel('n_estimators')\n",
        "    plt.ylabel('auc')\n",
        "\n",
        "    if (not overwrite) and os.path.exists('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png'):\n",
        "        plt.show()\n",
        "        return False\n",
        "    plt.savefig('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the feature importance figures of the model\n",
        "def plot_importance_custom(model, importance_type='weight',plot_bar_label=False, plot_importance_label=True, max_num_features=-1):\n",
        "    plt.close()\n",
        "    fig, ax = plt.subplots(figsize=(15, 15))\n",
        "    # Plot the feature importance histogram\n",
        "    if plot_bar_label:\n",
        "        plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the XGBoost built-in plot_importance figure\n",
        "    if plot_importance_label:\n",
        "        if max_num_features == -1:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type)\n",
        "        else:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type, max_num_features=max_num_features)\n",
        "        plt.show()\n",
        "\n",
        "# Function to fit and predict the model\n",
        "# cross validation enabled to obtain the best n_estimators if useTrainCV==True\n",
        "def model_fit(model, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "    global X_train,Y_train,training_year\n",
        "    if useTrainCV:\n",
        "        xgb_param = model.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(X_train.values, label=Y_train.values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
        "                          metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=50)\n",
        "        n_estimators=cvresult.shape[0]\n",
        "        model.set_params(n_estimators=n_estimators)\n",
        "\n",
        "        save_to_file(cvresult,'cvresult_'+str(n_estimators)+'_'+str(model.learning_rate)+'_'+str(training_year),'Database/result/'+str(training_year)+'/'+'n_estimators',overwrite=False)\n",
        "        plot_cvresult(cvresult,n_estimators,model.learning_rate)\n",
        "\n",
        "    # Fit the model on the data\n",
        "    model.fit(X_train, Y_train, eval_metric='auc')\n",
        "\n",
        "    # Predict the training set:\n",
        "    Y_pred = model.predict(X_train)\n",
        "    Y_predprob = model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "    # Print the model report:\n",
        "    print(\"\\nModel Report\")\n",
        "    print(\"Accuracy : %.4g\" % accuracy_score(Y_train.values, Y_pred))\n",
        "    print(\"AUC Score (Train): %f\" % roc_auc_score(Y_train, Y_predprob))\n",
        "\n",
        "    plot_importance_custom(model)\n",
        "\n",
        "# Print the grid search cv results\n",
        "def print_grid_search_cv_results_(grid_search_model):\n",
        "    for index, value in enumerate(grid_search_model.cv_results_['params']):\n",
        "        print(value, \" : \", '%.4f' % grid_search_model.cv_results_['mean_test_score'][index], ' , ',\n",
        "              '%.4f' % grid_search_model.cv_results_['std_test_score'][index])\n",
        "    print('Best Params: ', grid_search_model.best_params_)\n",
        "    print(grid_search_model.best_score_)\n",
        "\n",
        "# Plot the grid search auc figure given the model\n",
        "def plot_grid_search_auc(grid_search_model,para_list,overwrite=False,plot_label=True):\n",
        "    plt.close()\n",
        "    # If there is only 1 parameter in the para_list\n",
        "    if len(para_list)==1:\n",
        "        para_label=list(para_list.keys())[0]\n",
        "        para_range=list(para_list.values())[0]\n",
        "        test_means = grid_search_model.cv_results_['mean_test_score']\n",
        "        test_scores = np.array(test_means)\n",
        "\n",
        "        #print(para_range,test_scores,para_label,sep='\\n')\n",
        "        plt.plot(para_range, test_scores, label=para_label)\n",
        "        plt.legend()\n",
        "        plt.xlabel(para_label)\n",
        "        plt.ylabel('auc')\n",
        "    # Two parameters in the para_list\n",
        "    else:\n",
        "        x_label=list(para_list.keys())[1]\n",
        "        y_label=list(para_list.keys())[0]\n",
        "        x_len=len(list(para_list.values())[1])\n",
        "        y_len=len(list(para_list.values())[0])\n",
        "\n",
        "        grid_visualisation=[]\n",
        "        for pair in grid_search_model.cv_results_['mean_test_score']:\n",
        "            grid_visualisation.append(pair)\n",
        "        grid_visualisation=np.array(grid_visualisation)\n",
        "        grid_visualisation.shape=(y_len,x_len)\n",
        "\n",
        "        sns.heatmap(grid_visualisation,cmap='Blues')\n",
        "\n",
        "        # Switch the x and y axis if length are equal\n",
        "        if not x_len==y_len:\n",
        "            plt.xticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.yticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.xlabel(x_label)\n",
        "            plt.ylabel(y_label)\n",
        "        else:\n",
        "            plt.xticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.yticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.xlabel(y_label)\n",
        "            plt.ylabel(x_label)\n",
        "        \n",
        "    path='Database/result/'+str(training_year)+'/' + str(list(para_list.keys()))+'_'+str(list(para_list.values()))+'_'+str(training_year) + '.png'\n",
        "    if (not overwrite) and os.path.exists(path):\n",
        "        if plot_label:\n",
        "            plt.show()\n",
        "        return False\n",
        "    plt.title(str(list(para_list.keys()))+' vs auc')\n",
        "    plt.savefig(path)\n",
        "    if plot_label:\n",
        "        plt.show()\n",
        "\n",
        "# Grid search on the parameters\n",
        "def grid_search_on_para(para_list,num,re_run=False,print_results=False,overwrite=False):\n",
        "    global xgb_model,X_train,Y_train\n",
        "\n",
        "    if (re_run==False) and (os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')):\n",
        "        grid_search_model = read_from_file('grid_search_model'+str(num)+'_'+str(training_year), 'Database/result/'+str(training_year))\n",
        "\n",
        "    if (re_run) or (not os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')) or re_run:\n",
        "        global xgb_model,X_train,Y_train\n",
        "        grid_search_model = GridSearchCV(estimator = xgb_model,\n",
        "                               param_grid = para_list, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=1)\n",
        "        grid_search_model.fit(X_train,Y_train)\n",
        "        save_to_file(grid_search_model,'grid_search_model'+str(num)+'_'+str(training_year),'Database/result/'+str(training_year),overwrite=True)\n",
        "    # Print results\n",
        "    if print_results:\n",
        "        print_grid_search_cv_results_(grid_search_model)\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite)\n",
        "    else:\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite,plot_label=False)\n",
        "\n",
        "    return grid_search_model\n",
        "\n",
        "# Return the model auc or acc on test set\n",
        "def get_model_accuracy(model,prob_label=False):\n",
        "    global X_train,Y_train,X_test,Y_gold\n",
        "    model.fit(X_train,Y_train)\n",
        "    Y_test=model.predict(X_test)\n",
        "    Y_prob=model.predict_proba(X_test)\n",
        "    accuracy=accuracy_score(Y_gold,Y_test)\n",
        "    if prob_label:\n",
        "        return accuracy,Y_prob\n",
        "    else:\n",
        "        return accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pro=authorization_jq(1)\n",
        "\n",
        "    training_year=2\n",
        "    interval=20\n",
        "    start_date = '2007-01-01'\n",
        "    end_date = '2020-07-15'\n",
        "    #base_date = '2015-12-31'\n",
        "    base_date='2017-01-24'\n",
        "    date_list = get_date_list(start_date, end_date, base_date, interval=interval)\n",
        "\n",
        "    training_set, test_set = get_dataset_bydate(base_date,date_list,year=training_year)\n",
        "    X_train=training_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_train=training_set['label']\n",
        "    #Y_train=training_set['label'].apply(lambda x:0 if x==-1 else 1)\n",
        "    X_test=test_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_gold=test_set['label']\n",
        "    #Y_gold=test_set['label'].apply(lambda x:0 if x==-1 else 1)\n",
        "\n",
        "    xgb_model = XGBClassifier(\n",
        "            learning_rate =0.1,\n",
        "            n_estimators=200,\n",
        "            max_depth=5,\n",
        "            min_child_weight=1,\n",
        "            gamma=0,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective= 'binary:logistic',\n",
        "            nthread=-1,\n",
        "            scale_pos_weight=1,\n",
        "            seed=27,\n",
        "            tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "            )\n",
        "\n",
        "    # Grid search on learning_rate\n",
        "    para_list10 = {\n",
        "    'learning_rate':[0.05,0.1,0.15,0.2,0.3]\n",
        "    }\n",
        "    grid_search_on_para(para_list10,10)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list11 = {\n",
        "    'learning_rate':[i/100.0 for i in range(15,30,1)],\n",
        "    }\n",
        "    grid_search_on_para(para_list11,11)\n",
        "\n",
        "    xgb_model.learning_rate=0.24\n",
        "    xgb_model.n_estimators=1000\n",
        "\n",
        "    # Find the optimal number of n_estimators\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    xgb_model.n_estimators=200\n",
        "    # Grid search on max_depth and min_child_weight\n",
        "    para_list1 = {\n",
        "        'max_depth': range(3, 10, 2),\n",
        "        'min_child_weight': range(1, 6, 2)\n",
        "    }\n",
        "    grid_search_on_para(para_list1,1)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list2 = {\n",
        "        'max_depth': [6,7,8],\n",
        "        'min_child_weight': [1,2,3,4,5]\n",
        "    }\n",
        "    grid_search_on_para(para_list2,2)\n",
        "\n",
        "    xgb_model.max_depth=8\n",
        "    xgb_model.min_child_weight=2\n",
        "\n",
        "    # Grid search on gamma\n",
        "    para_list3 = {\n",
        "        'gamma':[i/10.0 for i in range(0,5)]\n",
        "    }\n",
        "    grid_search_on_para(para_list3,3)\n",
        "\n",
        "    # Grid search on subsample and colsample_bytree\n",
        "    para_list4 = {\n",
        "        'subsample':[i/10.0 for i in range(6,10)],\n",
        "        'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
        "    }   \n",
        "    grid_search_on_para(para_list4,4)\n",
        "    #print(get_model_accuracy(xgb_model))\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list5 = {\n",
        "        'subsample':[i/100.0 for i in range(70,91,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(70,91,5)],\n",
        "    }   \n",
        "    grid_search_on_para(para_list5,5)\n",
        "\n",
        "    xgb_model.subsample=0.8\n",
        "    xgb_model.colsample_bytree=0.8\n",
        "\n",
        "    # Grid search on reg_aplha\n",
        "    para_list6 = {\n",
        "    'reg_alpha':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list6,6)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list7 = {\n",
        "    'reg_alpha':[1e-6,5e-5,1e-5,5e-4,1e-4]\n",
        "    }\n",
        "    grid_search_on_para(para_list7,7)\n",
        "    #grid_search_on_para(para_list7,7,re_run=True,print_results=True,overwrite=True)\n",
        "\n",
        "    xgb_model.reg_alpha=1e-6\n",
        "\n",
        "    # Grid search on reg_lamda\n",
        "    para_list8 = {\n",
        "    'reg_lambda':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list8,8)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list9 = {\n",
        "    'reg_lambda':[0.5,0.8,1,1.5,2,5,10]\n",
        "    }\n",
        "    grid_search_on_para(para_list9,9)\n",
        "\n",
        "    xgb_model.kwargs['reg_lamda']=1\n",
        "\n",
        "    xgb_model.n_estimators=5000\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    # Final model concerning the 2-year training set\n",
        "    xgb_model = XGBClassifier(\n",
        "        learning_rate =0.24,\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        min_child_weight=2,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=1e-6,\n",
        "        reg_lamda=1,\n",
        "\n",
        "        objective= 'binary:logistic',\n",
        "        nthread=-1,\n",
        "        scale_pos_weight=1,\n",
        "        seed=27,\n",
        "        tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "        )\n",
        "    print(get_model_accuracy(xgb_model))\n",
        "\n",
        "plt.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vft8K958Bzsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take the previous 3-year data as the training set and generate the corresponding optimal model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV,cross_validate\n",
        "\n",
        "from Data_Cleaning import save_to_file,read_from_file,get_dataset_bydate,get_date_list,authorization_jq\n",
        "\n",
        "# Plot the \"n_estimators vs auc\" curve figure\n",
        "def plot_cvresult(cvresult,n_estimators,learning_rate,split_part=100,overwrite=False):\n",
        "    plt.close()\n",
        "    global training_year\n",
        "    split_num=round(n_estimators/split_part)\n",
        "    # plot\n",
        "    test_means = cvresult['test-auc-mean'][::split_num]\n",
        "    test_stds = cvresult['test-auc-std'][::split_num]\n",
        "\n",
        "    train_means = cvresult['train-auc-mean'][::split_num]\n",
        "    train_stds = cvresult['train-auc-std'][::split_num]\n",
        "\n",
        "    x_axis = range(0, n_estimators)[::split_num]\n",
        "    plt.errorbar(x_axis, test_means, yerr=test_stds, label='Test')\n",
        "    plt.errorbar(x_axis, train_means, yerr=train_stds, label='Train')\n",
        "    plt.title(\"n_estimators vs auc\")\n",
        "    plt.xlabel('n_estimators')\n",
        "    plt.ylabel('auc')\n",
        "\n",
        "    if (not overwrite) and os.path.exists('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png'):\n",
        "        plt.show()\n",
        "        return False\n",
        "    plt.savefig('Database/result/'+str(training_year)+'/'+'n_estimators_'+str(learning_rate)+'_'+str(training_year)+'.png')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the feature importance figures of the model\n",
        "def plot_importance_custom(model, importance_type='weight',plot_bar_label=False, plot_importance_label=True, max_num_features=-1):\n",
        "    plt.close()\n",
        "    fig, ax = plt.subplots(figsize=(15, 15))\n",
        "    # Plot the feature importance histogram\n",
        "    if plot_bar_label:\n",
        "        plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the XGBoost built-in plot_importance figure\n",
        "    if plot_importance_label:\n",
        "        if max_num_features == -1:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type)\n",
        "        else:\n",
        "            plot_importance(model, height=0.5, ax=ax, importance_type=importance_type, max_num_features=max_num_features)\n",
        "        plt.show()\n",
        "\n",
        "# Function to fit and predict the model\n",
        "# cross validation enabled to obtain the best n_estimators if useTrainCV==True\n",
        "def model_fit(model, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "    global X_train,Y_train,training_year\n",
        "    if useTrainCV:\n",
        "        xgb_param = model.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(X_train.values, label=Y_train.values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
        "                          metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=50)\n",
        "        n_estimators=cvresult.shape[0]\n",
        "        model.set_params(n_estimators=n_estimators)\n",
        "\n",
        "        save_to_file(cvresult,'cvresult_'+str(n_estimators)+'_'+str(model.learning_rate)+'_'+str(training_year),'Database/result/'+str(training_year)+'/'+'n_estimators',overwrite=False)\n",
        "        plot_cvresult(cvresult,n_estimators,model.learning_rate)\n",
        "\n",
        "    # Fit the model on the data\n",
        "    model.fit(X_train, Y_train, eval_metric='auc')\n",
        "\n",
        "    # Predict the training set:\n",
        "    Y_pred = model.predict(X_train)\n",
        "    Y_predprob = model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "    # Print the model report:\n",
        "    print(\"\\nModel Report\")\n",
        "    print(\"Accuracy : %.4g\" % accuracy_score(Y_train.values, Y_pred))\n",
        "    print(\"AUC Score (Train): %f\" % roc_auc_score(Y_train, Y_predprob))\n",
        "\n",
        "    plot_importance_custom(model)\n",
        "\n",
        "# Print the grid search cv results\n",
        "def print_grid_search_cv_results_(grid_search_model):\n",
        "    for index, value in enumerate(grid_search_model.cv_results_['params']):\n",
        "        print(value, \" : \", '%.4f' % grid_search_model.cv_results_['mean_test_score'][index], ' , ',\n",
        "              '%.4f' % grid_search_model.cv_results_['std_test_score'][index])\n",
        "    print('Best Params: ', grid_search_model.best_params_)\n",
        "    print(grid_search_model.best_score_)\n",
        "\n",
        "# Plot the grid search auc figure given the model\n",
        "def plot_grid_search_auc(grid_search_model,para_list,overwrite=False,plot_label=True):\n",
        "    plt.close()\n",
        "    # If there is only 1 parameter in the para_list\n",
        "    if len(para_list)==1:\n",
        "        para_label=list(para_list.keys())[0]\n",
        "        para_range=list(para_list.values())[0]\n",
        "        test_means = grid_search_model.cv_results_['mean_test_score']\n",
        "        test_scores = np.array(test_means)\n",
        "\n",
        "        #print(para_range,test_scores,para_label,sep='\\n')\n",
        "        plt.plot(para_range, test_scores, label=para_label)\n",
        "        plt.legend()\n",
        "        plt.xlabel(para_label)\n",
        "        plt.ylabel('auc')\n",
        "    # Two parameters in the para_list\n",
        "    else:\n",
        "        x_label=list(para_list.keys())[1]\n",
        "        y_label=list(para_list.keys())[0]\n",
        "        x_len=len(list(para_list.values())[1])\n",
        "        y_len=len(list(para_list.values())[0])\n",
        "\n",
        "        grid_visualisation=[]\n",
        "        for pair in grid_search_model.cv_results_['mean_test_score']:\n",
        "            grid_visualisation.append(pair)\n",
        "        grid_visualisation=np.array(grid_visualisation)\n",
        "        grid_visualisation.shape=(y_len,x_len)\n",
        "\n",
        "        sns.heatmap(grid_visualisation,cmap='Blues')\n",
        "\n",
        "        # Switch the x and y axis if length are equal\n",
        "        if not x_len==y_len:\n",
        "            plt.xticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.yticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.xlabel(x_label)\n",
        "            plt.ylabel(y_label)\n",
        "        else:\n",
        "            plt.xticks(np.arange(y_len)+0.5,grid_search_model.param_grid[y_label])\n",
        "            plt.yticks(np.arange(x_len)+0.5,grid_search_model.param_grid[x_label])\n",
        "            plt.xlabel(y_label)\n",
        "            plt.ylabel(x_label)\n",
        "        \n",
        "    path='Database/result/'+str(training_year)+'/' + str(list(para_list.keys()))+'_'+str(list(para_list.values()))+'_'+str(training_year) + '.png'\n",
        "    if (not overwrite) and os.path.exists(path):\n",
        "        if plot_label:\n",
        "            plt.show()\n",
        "        return False\n",
        "    plt.title(str(list(para_list.keys()))+' vs auc')\n",
        "    plt.savefig(path)\n",
        "    if plot_label:\n",
        "        plt.show()\n",
        "\n",
        "# Grid search on the parameters\n",
        "def grid_search_on_para(para_list,num,re_run=False,print_results=False,overwrite=False):\n",
        "    global xgb_model,X_train,Y_train\n",
        "\n",
        "    if (re_run==False) and (os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')):\n",
        "        grid_search_model = read_from_file('grid_search_model'+str(num)+'_'+str(training_year), 'Database/result/'+str(training_year))\n",
        "\n",
        "    if (re_run) or (not os.path.exists('Database/result/'+str(training_year)+'/'+'grid_search_model'+str(num)+'_'+str(training_year)+'.pkl')) or re_run:\n",
        "        global xgb_model,X_train,Y_train\n",
        "        grid_search_model = GridSearchCV(estimator = xgb_model,\n",
        "                               param_grid = para_list, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=1)\n",
        "        grid_search_model.fit(X_train,Y_train)\n",
        "        save_to_file(grid_search_model,'grid_search_model'+str(num)+'_'+str(training_year),'Database/result/'+str(training_year),overwrite=True)\n",
        "    # Print results\n",
        "    if print_results:\n",
        "        print_grid_search_cv_results_(grid_search_model)\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite)\n",
        "    else:\n",
        "        plot_grid_search_auc(grid_search_model,para_list,overwrite,plot_label=False)\n",
        "\n",
        "    return grid_search_model\n",
        "\n",
        "# Return the model auc or acc on test set\n",
        "def get_model_accuracy(model,prob_label=False):\n",
        "    global X_train,Y_train,X_test,Y_gold\n",
        "    model.fit(X_train,Y_train)\n",
        "    Y_test=model.predict(X_test)\n",
        "    Y_prob=model.predict_proba(X_test)\n",
        "    accuracy=accuracy_score(Y_gold,Y_test)\n",
        "    if prob_label:\n",
        "        return accuracy,Y_prob\n",
        "    else:\n",
        "        return accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pro=authorization_jq(1)\n",
        "\n",
        "    training_year=3\n",
        "    interval=20\n",
        "    start_date = '2007-01-01'\n",
        "    end_date = '2020-07-15'\n",
        "    #base_date = '2015-12-31'\n",
        "    base_date='2017-01-24'\n",
        "    date_list = get_date_list(start_date, end_date, base_date, interval=interval)\n",
        "\n",
        "    training_set, test_set = get_dataset_bydate(base_date,date_list,year=training_year)\n",
        "    X_train=training_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_train=training_set['label']\n",
        "    X_test=test_set.drop(['pchg', 'label'], axis=1)\n",
        "    Y_gold=test_set['label']\n",
        "\n",
        "    xgb_model = XGBClassifier(\n",
        "            learning_rate =0.1,\n",
        "            n_estimators=1000,\n",
        "            max_depth=5,\n",
        "            min_child_weight=1,\n",
        "            gamma=0,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective= 'binary:logistic',\n",
        "            nthread=-1,\n",
        "            scale_pos_weight=1,\n",
        "            seed=27,\n",
        "            tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "            )\n",
        "\n",
        "    # Find the optimal number of n_estimators\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    xgb_model.n_estimators=200\n",
        "    # Grid search on max_depth and min_child_weight\n",
        "    para_list1 = {\n",
        "        'max_depth': range(3, 10, 2),\n",
        "        'min_child_weight': range(1, 6, 2)\n",
        "    }\n",
        "    grid_search_on_para(para_list1,1)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list2 = {\n",
        "        'max_depth': [6,7,8],\n",
        "        'min_child_weight': [1,2,3,4,5]\n",
        "    }\n",
        "    grid_search_on_para(para_list2,2)\n",
        "\n",
        "    xgb_model.max_depth=7\n",
        "    xgb_model.min_child_weight=4\n",
        "\n",
        "    # Grid search on gamma\n",
        "    para_list3 = {\n",
        "        'gamma':[i/10.0 for i in range(0,5)]\n",
        "    }\n",
        "    grid_search_on_para(para_list3,3)\n",
        "\n",
        "    # Grid search on subsample and colsample_bytree\n",
        "    para_list4 = {\n",
        "        'subsample':[i/10.0 for i in range(6,10)],\n",
        "        'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
        "    }   \n",
        "    grid_search_on_para(para_list4,4)\n",
        "    #print(model.best_estimator_)\n",
        "    #model=model.best_estimator_\n",
        "    #print(get_model_accuracy(model))\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list5 = {\n",
        "        'subsample':[i/100.0 for i in range(70,91,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(80,100,5)],\n",
        "    }   \n",
        "    grid_search_on_para(para_list5,5)\n",
        "\n",
        "    xgb_model.subsample=0.75\n",
        "    xgb_model.colsample_bytree=0.85\n",
        "\n",
        "    # Grid search on reg_aplha\n",
        "    para_list6 = {\n",
        "    'reg_alpha':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list6,6)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list7 = {\n",
        "    'reg_alpha':[1e-6,5e-5,1e-5,5e-4,1e-4]\n",
        "    }\n",
        "    grid_search_on_para(para_list7,7)\n",
        "    #grid_search_on_para(para_list7,7,re_run=True,print_results=True,overwrite=True)\n",
        "\n",
        "    xgb_model.reg_alpha=1e-5\n",
        "\n",
        "    # Grid search on reg_lamda\n",
        "    para_list8 = {\n",
        "    'reg_lambda':[1e-5,1e-2,0.1,1,100]\n",
        "    }\n",
        "    grid_search_on_para(para_list8,8)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list9 = {\n",
        "    'reg_lambda':[0.5,0.8,1,1.5,2,5,10]\n",
        "    }\n",
        "    grid_search_on_para(para_list9,9)\n",
        "\n",
        "    xgb_model.kwargs['reg_lamda']=0.8\n",
        "\n",
        "    # Grid search on learning_rate\n",
        "    para_list10 = {\n",
        "    'learning_rate':[0.05,0.1,0.15,0.2,0.3]\n",
        "    }\n",
        "    grid_search_on_para(para_list10,10,print_results=True)\n",
        "\n",
        "    # Narrow the search\n",
        "    para_list11 = {\n",
        "    'learning_rate':[i/100.0 for i in range(8,13,1)],\n",
        "    }\n",
        "    grid_search_on_para(para_list11,11,print_results=True)\n",
        "    \n",
        "    xgb_model.learning_rate=0.1\n",
        "    xgb_model.n_estimators=5000\n",
        "\n",
        "    #model_fit(xgb_model)\n",
        "\n",
        "    # Final model concerning the 3-year training set\n",
        "    xgb_model = XGBClassifier(\n",
        "        learning_rate =0.1,\n",
        "        n_estimators=200,\n",
        "        max_depth=7,\n",
        "        min_child_weight=4,\n",
        "        gamma=0,\n",
        "        subsample=0.75,\n",
        "        colsample_bytree=0.85,\n",
        "        reg_alpha=1e-5,\n",
        "        reg_lamda=0.8,\n",
        "\n",
        "        objective= 'binary:logistic',\n",
        "        nthread=-1,\n",
        "        scale_pos_weight=1,\n",
        "        seed=27,\n",
        "        tree_method=\"gpu_hist\" # Gpu boost enable\n",
        "        )\n",
        "    print(get_model_accuracy(xgb_model))\n",
        "\n",
        "    plt.close()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}